# A cromwell config file to replace the default docker runner with singularity and add SLURM
# Modified and based on:
# https://gist.github.com/illusional/b70f870fa0e2f8e7a0ba0a9e71d568f5
# https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/singularity.slurm.conf
# https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/slurm.conf

include required(classpath("application"))

backend {
  default: slurm-singularity
  providers: {
    singularity {
      # The backend custom configuration.
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"

      config {
        run-in-background = true
        runtime-attributes = """
                  String? docker
		            """
        submit = """
                    ${job_shell} ${script}
		            """
        submit-docker = """
                  singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}
                """
      }
    }

    SLURM {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        runtime-attributes = """
                Int runtime_minutes = 30
                Int runtime_hours = 0
                Int runtime_days = 0
                Int cpus = 2
                Int requested_memory_mb_per_core = 8000
                String queue = "short"
                """

        submit = """
                    sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_days}-${runtime_hours}:${runtime_minutes} -p ${queue} \
                    ${"-c " + cpus} \
                    --mem-per-cpu ${requested_memory_mb_per_core} \
                    --wrap "/bin/bash ${script}"
                """
        kill = "scancel ${job_id}"
        check-alive = "squeue -j ${job_id}"
        job-id-regex = "Submitted batch job (\\d+).*"
      }
    }
    "slurm-singularity": {
      "actor-factory": "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory",
      "config": {
        "filesystems": {
          "local": {
            "localization": [
              "hard-link",
              "cached-copy"
            ],
            "enabled": true,
            "caching": {
              "duplication-strategy": [
                "hard-link",
                "cached-copy",
                "copy",
                "soft-link"
              ],
              "hashing-strategy": "fingerprint"
            }
          }
        },
        "runtime-attributes": """
    Int runtime_minutes = 30
    Int runtime_hours = 0
    Int runtime_days = 0
    Int? cpu = 2
    Int requested_memory_mb_per_core = 8000
    String? docker
    String? queue
    String cacheLocation = "~/.cache"
    """,

        "submit": """
    jobname='${sub(sub(cwd, ".*call-", ""), "/", "-")}-cpu-${cpu}-mempercore-${requested_memory_mb_per_core}'
    sbatch \
        -J $jobname \
        -D ${cwd} \
        -o ${out} \
        -e ${err} \
        -t ${runtime_days}-${runtime_hours}:${runtime_minutes} \
        ${"-p " + queue} \
        ${"-n " + cpu} \
        --mem-per-cpu ${requested_memory_mb_per_core} \
        --wrap "/usr/bin/env ${job_shell} ${script}"
    """,
        "submit-docker": """

    docker_subbed=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker})
    image=${cacheLocation}/$docker_subbed.sif
    lock_path=${cacheLocation}/$docker_subbed.lock

    if [ ! -f "$image" ]; then
      singularity pull $image docker://${docker}
    fi

    # Submit the script to SLURM
    jobname=${sub(sub(cwd, ".*call-", ""), "/", "-")}-cpu-${cpu}-mempercore-${requested_memory_mb_per_core}
    JOBID=$(sbatch \
        --parsable \
        -J $jobname \
        --mem-per-cpu ${requested_memory_mb_per_core} \
        --cpus-per-task ${select_first([cpu, 1])} \
        ${"-p " + queue} \
        -D ${cwd} \
        -o ${cwd}/execution/stdout \
        -e ${cwd}/execution/stderr \
        -t ${runtime_days}-${runtime_hours}:${runtime_minutes} \
        --wrap "singularity exec --bind ${cwd}:${docker_cwd} $image ${job_shell} ${docker_script}") \
         && NTOKDEP=$(sbatch --parsable --kill-on-invalid-dep=yes --dependency=afternotok:$JOBID --wrap '[ ! -f rc ] && (echo 1 >> ${cwd}/execution/rc) && (echo "A slurm error occurred" >> ${cwd}/execution/stderr)') \
        && echo Submitted batch job $JOBID""",
        "kill": "scancel ${job_id}",
        "check-alive": "scontrol show job ${job_id}",
        "job-id-regex": "Submitted batch job (\\d+).*"
      }
    }
  }
}

